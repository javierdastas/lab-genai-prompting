{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# LAB | GenAI: Exploring Prompting Techniques for Customer Support Automation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks\n",
    "\n",
    "**Objective:**\n",
    "\n",
    "Learn and apply different prompting techniques to improve the performance of a language model in generating customer support responses.\n",
    "\n",
    "**Business Case:**\n",
    "\n",
    "Imagine you are working for a company that provides a variety of services, including technical support, billing inquiries, and general customer service. Your task is to use a language model to automate responses to customer emails.\n",
    "\n",
    "**Dataset:**\n",
    "\n",
    "Download the FAQ of a company to do this exercise. Below you have a couple of examples, but feel free to find your own:\n",
    " - https://info.undp.org/erecruit/documents/FAQ.pdf\n",
    " - https://www.cambridgeenglish.org/Images/696254-faqs-digital-cambridge-english-qualifications.pdf\n",
    " - https://www.wscc.nt.ca/sites/default/files/documents/0009-518-Item-04-INDESIGN-FAQ-Template%203%20-%20MINUS%20FIRST%20QUESTION.pdf\n",
    "\n",
    "\n",
    "### Task 1\n",
    "\n",
    "Download and Read the PDF:\n",
    "\n",
    "  - Choose one of the provided FAQ PDFs or find your own relevant FAQ document.\n",
    "  - Read through the FAQ document carefully to understand the types of questions and answers it contains.\n",
    "  - Create Questions Based on the PDF ( you can use ChatGPT for this)\n",
    "    - Generate a list of potential customer questions that could be answered using the information from the FAQ PDF.\n",
    "    - Ensure your questions cover a variety of topics and difficulty levels found within the document.\n",
    "    - Generate Responses Using Different Prompting Techniques:\n",
    "\n",
    "Use a language model (such as ChatGPT) to generate responses to your questions.\n",
    "Experiment with different prompting techniques to see how they affect the quality of the responses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Types of prompting\n",
    "\n",
    "For each of the types prompting, perform the following:\n",
    " - Research what the type of prompting is\n",
    " - Create a small explaination of the prompting\n",
    " - Test your type of prompting vs the control prompt (direct question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Zero-Shot Prompting\n",
    "\n",
    "Use the knowledge base to create prompts without examples.\n",
    "Test the model's ability to generate accurate responses based solely on the provided instructions.\n",
    "Assess the performance compared to few-shot prompting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Few-Shot Prompting\n",
    "\n",
    "Select a few representative emails from each category.\n",
    "Create prompts by including these examples and ask the model to generate responses for new emails.\n",
    "Evaluate the quality and relevance of the responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chain of Thought Prompting\n",
    "\n",
    "Develop prompts that guide the model to think through the problem step-by-step before providing the final answer.\n",
    "Analyze if this approach improves the quality of technical support responses.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instruction-Based Prompting\n",
    "\n",
    "Write clear and explicit instructions in the prompts for each type of customer inquiry.\n",
    "Measure the effectiveness of detailed instructions in guiding the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Role-Playing Prompting\n",
    "\n",
    "Ask the model to respond as a customer service representative or technical support expert.\n",
    "Evaluate how well the model adopts the role and provides relevant information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contextual Prompting\n",
    "\n",
    "Provide relevant context from previous email threads or the knowledge base before posing the main question.\n",
    "Test if providing context improves the accuracy and relevance of the responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conversational Prompting\n",
    "\n",
    "Create a dialogue-style prompt where the model continues an ongoing conversation with the customer.\n",
    "Observe how well the model maintains context and coherence in multi-turn conversations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contrastive Prompting\n",
    "\n",
    "Show the model examples of both good and bad responses.\n",
    "Use these contrasting examples to guide the model towards generating better responses.\n",
    "Compare the results with other techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specificity Prompting\n",
    "\n",
    "Ask the model to respond with a specific style, tone, or level of detail, such as formal, friendly, or concise.\n",
    "Assess how well the model adapts its responses to the specified requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterative Refinement Prompting\n",
    "\n",
    "Ask the model to refine or improve upon its previous response.\n",
    "Experiment with multiple iterations to see if responses improve over time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_ironhack",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
